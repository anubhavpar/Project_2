{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code Section for Declaring Imports##\n",
    "\n",
    "from bs4 import BeautifulSoup # Load the libraries for beautiful soup\n",
    "import requests\n",
    "#import urllib2\n",
    "import time \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Code Section for Beautiful Soup Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code Section for Beautiful Soup Headers##\n",
    "\n",
    "headers = {'User-Agent': \n",
    "           'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "\n",
    "page = \"https://fbref.com/en/comps/9/1526/2016-2017-Premier-League-Stats\"\n",
    "pageTree = requests.get(page, headers=headers)\n",
    "pageSoup_teams = BeautifulSoup(pageTree.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Code Section to fetch the Teams - Source : Fbref.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code Section to fetch the Teams ##\n",
    "\n",
    "Player_Teams = []\n",
    "table = pageSoup_teams.find('table', class_ = 'min_width sortable stats_table')\n",
    "for link in table.find_all('a'):\n",
    "        temp = \"https://fbref.com\" + link.get(\"href\")\n",
    "        team_name = link.text\n",
    "        Player_Teams.append([team_name,temp])\n",
    "Player_Teams_upd = []\n",
    "for value in Player_Teams:\n",
    "    if value[1][-1:-6:-1] == 'statS':\n",
    "        Player_Teams_upd.append(value)\n",
    "        \n",
    "Player_Teams_upd;        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Code Section to fetch the Players from Teams - Source : Fbref.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code Section to fetch Player List from Teams ##\n",
    "\n",
    "Players_Info = {}\n",
    "for player_page in Player_Teams_upd:\n",
    "    page = player_page[1]\n",
    "    player_team = player_page[0]\n",
    "    pageTree = requests.get(page, headers=headers)\n",
    "    pageSoup_teams = BeautifulSoup(pageTree.content, 'html.parser')\n",
    "    \n",
    "    for table in pageSoup_teams.find_all('div', id = 'div_stats_standard_1526'):\n",
    "        rows = [row for row in table.find_all('tr')]\n",
    "    for index in range(2,len(rows)-2):\n",
    "        player_name =     rows[index].find_all('th')[0].find('a').text\n",
    "        player_url  =     'https://fbref.com' + rows[index].find_all('th')[0].find('a')['href']\n",
    "        player_details =  rows[index].find_all('td')\n",
    "        Players_Info[player_name] = [player_name] + [player_url] + [player_team] + [i.text for i in player_details[:-1]] \n",
    "\n",
    "Players_Info_df = pd.DataFrame(Players_Info).T  #transpose\n",
    "Players_Info_df.columns = ['Player','PlLink','Team','Nation', 'Pos','Age','MP','Starts','Min','90s','Gls','Ast', \n",
    "                    'PK','PKatt','CrdY','CrdR','PGls','PAst','PG+A','PG-PK','PG+A-PK'] \n",
    "Players_Info_df.set_index('Player', inplace= True)\n",
    "Players_Info_df;        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Code Section to fetch Player level Attributes - Source : Fbref.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code Section to fetch Specific Player Attributes##\n",
    "Player_Attr = {}\n",
    "Player_Shooting = {}\n",
    "for index, row in enumerate(Players_Info_df.head(2).itertuples()):\n",
    "    page = row.PlLink\n",
    "    pageTree = requests.get(page, headers=headers)\n",
    "    pageSoup_player = BeautifulSoup(pageTree.content, 'html.parser')\n",
    "\n",
    "    #pageSoup_player \n",
    "    for table in pageSoup_player.find_all('div', class_ = 'players', id = 'info'):\n",
    "        #print(table)\n",
    "        rows = [row for row in table.find_all('p')]\n",
    "        headings = [heading for heading in table.find_all('h1') ]\n",
    "    try:   \n",
    "        #print(headings[0])\n",
    "        player_name   = headings[0].find('span').text\n",
    "        #print(player_name)\n",
    "        player_height = rows[2].find_all('span')[0].text\n",
    "        player_weight = rows[2].find_all('span')[1].text\n",
    "        player_footedness = rows[1].text\n",
    "        Player_Attr[player_name] = [player_name] + [player_height] + [player_weight] + [player_footedness]\n",
    "    except:\n",
    "        player_name   = 'Error'\n",
    "        player_height = 'Error'\n",
    "        player_weight = 'Error'\n",
    "        player_footedness = 'Error'\n",
    "        Player_Attr[player_name] = [player_name] + [player_height] + [player_weight] + [player_footedness]\n",
    "        continue\n",
    "#    for table in pageSoup_player.find_all('div', id = 'div_stats_shooting_dom_lg'):\n",
    "#        rows = [row for row in table.find_all('tr')]\n",
    "        #for index in range(4:18): \n",
    "#        items = rows[12].find_all('td')\n",
    "#        Player_Shooting[player_name] = [player_name] + [i.text for i in items[4:]]\n",
    "\n",
    "#print(Player_Shooting)\n",
    "#Players_Shooting_df = pd.DataFrame(Player_Shooting).T \n",
    "#Players_Shooting_df.columns = ['Player','','Weight','Footedness']#print(rows[3])\n",
    "\n",
    "Players_Attr_df = pd.DataFrame(Player_Attr).T  #transpose\n",
    "Players_Attr_df.columns = ['Player','Height','Weight','Footedness'] \n",
    "Players_Attr_df.set_index('Player', inplace= True)\n",
    "## Merge the above different Player specific information into a single DataFrame for players\n",
    "Players_Info_outer_merged = pd.merge(Players_Info_df, Players_Attr_df,\n",
    "                        how=\"outer\", on=[\"Player\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code Section to fetch markvet value of the Players - Source : Transfermarkt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Beautiful Soup Header for Transfermark.com website(for market valuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code Section to fetch Player Market Valuations from TransferMarket Website##\n",
    "page = \"https://www.transfermarkt.us/premier-league/daten/wettbewerb/GB1\"\n",
    "pageTree = requests.get(page, headers=headers)\n",
    "pageSoup_teams = BeautifulSoup(pageTree.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fetch the teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams = []\n",
    "Teams_mod = []\n",
    "for table in pageSoup_teams.find_all('table', class_='items'):\n",
    "    #time.sleep(20)\n",
    "    for link in table.find_all('a'):\n",
    "        temp = \"https://www.transfermarkt.us\" + link.get(\"href\")\n",
    "        Teams.append(temp)\n",
    "  ## Notice that there are redundant links in the list so I am filtering out the duplicate values      \n",
    "    [Teams_mod.append(x) for x in Teams if x not in Teams_mod]\n",
    "\n",
    "Teams_corr = Teams_mod[4::2] #This is to clean-up redundant and un-necessary links in the data     \n",
    "Teams_corr;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fetch players from each of the teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams_Players_Names = []\n",
    "Teams_Players_Valuations = {}\n",
    "Teams_Players_Positions = []\n",
    "Teams_Players = []\n",
    "Teams_Players_mod = []\n",
    "team_page_temp = []\n",
    "player_valuations_dataframe = pd.DataFrame()\n",
    "#for team_pa'https://www.transfermarkt.us/chelsea-fc/startseite/verein/631?saison_id=20ge in Teams_corr: ## For each team in the list -> we go ahead to fetch the players\n",
    "test_page = \"https://www.transfermarkt.us/chelsea-fc/startseite/verein/631?saison_id=2016\"\n",
    "#team_page_temp.append(Teams_corr[0:3])\n",
    "team_page_temp.append(test_page)\n",
    "for team_page in team_page_temp: ## We Look through the list and fetch every team and load their respective pages \n",
    "    #time.sleep(20)\n",
    "    #print(team_page)\n",
    "    pageTree = requests.get(team_page, headers=headers)\n",
    "    pageSoup_player = BeautifulSoup(pageTree.content, 'html.parser')\n",
    "    for table in pageSoup_player.find_all('table', class_='items'):\n",
    "        for link in table.find_all('a', class_ = 'spielprofil_tooltip'):\n",
    "            if (link.get(\"title\")) not in Teams_Players_Names:\n",
    "                Teams_Players_Names.append(link.get(\"title\"))\n",
    "                #Player_Attr[link.get(\"title\")] = link.get(\"title\")\n",
    "        for link in table.find_all('td', class_ = 'rechts hauptlink'):\n",
    "                Teams_Players_Valuations.append(link.text)\n",
    "                \n",
    "Players_Attr_df = pd.DataFrame()  #transpose\n",
    "#Players_Attr_df.columns = ['Player','Valuation'] \n",
    "Players_Attr_df['Player'] = Teams_Players_Names\n",
    "Players_Attr_df['Valuation'] = Teams_Players_Valuations\n",
    "Players_Attr_df.set_index('Player', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Players_Attr_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fetch information from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pageviewapi\n",
    "page_views = pageviewapi.per_article('en.wikipedia', 'Rooney', '20151106', '20151120',\n",
    "                        access='all-access', agent='all-agents', granularity='daily')\n",
    "#print(page_views['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Merge with the Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Players_Dataset = pd.merge(Players_Attr_df,Players_Info_outer_merged,\n",
    "                        how=\"outer\", on=[\"Player\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Players_Dataset.groupby(\"Team\").head(25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
